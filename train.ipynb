{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import timm\n",
    "import torch\n",
    "import zipfile,os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\In AI We Trust\\\\GCD.zip'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), 'GCD.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_zip = os.path.join(os.getcwd(), 'GCD.zip')\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall(os.getcwd())\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\In AI We Trust\\GCD\\train\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(os.getcwd(), 'GCD', 'train')\n",
    "test_dir = os.path.join(os.getcwd(), 'GCD', 'test')\n",
    "\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.version.cuda)         # Should match the installed CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_model() got multiple values for argument 'pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxvit_tiny_tf_224.in1k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMaxViTSmall_i1k_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMaxViTBase_i1k_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# get model specific transforms (normalization, resize)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: create_model() got multiple values for argument 'pretrained'"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\n",
    "    'maxvit_tiny_tf_224.in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=7,\n",
    ")\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dir = Path(train_dir)\\n\\nfor img_path in train_dir.rglob(\"*.*\"):  # Recursively find all files\\n    if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:  # Check for valid image extensions\\n        img = Image.open(img_path).convert(\"RGB\")  # Open the image and convert to RGB\\n        img_tensor = transforms(img).unsqueeze(0)  # Preprocess and add batch dimension\\n\\n        # Pass the image through the model\\n        output = model.forward_features(transforms(img_tensor))\\n\\n        print(f\"Processed {img_path}: Output Shape - {output[-1].shape if isinstance(output, list) else output.shape}\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_dir = Path(train_dir)\n",
    "\n",
    "for img_path in train_dir.rglob(\"*.*\"):  # Recursively find all files\n",
    "    if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:  # Check for valid image extensions\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Open the image and convert to RGB\n",
    "        img_tensor = transforms(img).unsqueeze(0)  # Preprocess and add batch dimension\n",
    "\n",
    "        # Pass the image through the model\n",
    "        output = model.forward_features(transforms(img_tensor))\n",
    "\n",
    "        print(f\"Processed {img_path}: Output Shape - {output[-1].shape if isinstance(output, list) else output.shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for data in train_loader:\n",
    "#     print(data)  # This will show you the structure of the data being returned\n",
    "#     inputs, targets = data  # Unpack only if it has the correct structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = os.listdir(data_dir)\n",
    "\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label  # Ensure this returns a tuple of (image, label)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset = CustomDataset(data_dir=train_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "for data in train_loader:\n",
    "    print(data[0].shape)  # This will show you the structure of the data being returned\n",
    "    inputs, targets = data  # Unpack only if it has the correct structure\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: 100%|██████████| 1250/1250 [07:25<00:00,  2.81it/s, loss=1.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Avg Loss: 1.6947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|██████████| 1250/1250 [07:10<00:00,  2.91it/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Avg Loss: 1.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|██████████| 1250/1250 [07:10<00:00,  2.91it/s, loss=1.61] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Avg Loss: 1.1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|██████████| 1250/1250 [07:09<00:00,  2.91it/s, loss=1.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Avg Loss: 1.2079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|██████████| 1250/1250 [07:06<00:00,  2.93it/s, loss=0.954]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Avg Loss: 1.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "num_epochs = 2\n",
    "\n",
    "# Training loop\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0  # To accumulate loss for the epoch\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")  # Progress bar\n",
    "    \n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # Update progress bar description with the current loss\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImageFolder(root=test_dir, transform=transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. Define testing loop\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "\n",
    "'''\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for inputs, targets in test_loader:\n",
    "        # Perform forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get predicted class indices\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Compare predictions with ground truth\n",
    "        correct_predictions += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "        # Store predictions and labels for metrics\n",
    "        all_targets.extend(targets.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "'''\n",
    "# Perform forward pass\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Get predicted class indices\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "# Compare predictions with ground truth\n",
    "correct_predictions += (preds == targets).sum().item()\n",
    "total_samples += targets.size(0)\n",
    "\n",
    "# Store predictions and labels for metrics\n",
    "all_targets.extend(targets.tolist())\n",
    "all_preds.extend(preds.tolist())\n",
    "\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
